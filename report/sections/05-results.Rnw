\documentclass{article}

\begin{document}
\SweaveOpts{concordance=TRUE}

\section*{Results}

<<coefficients, results=tex, echo=FALSE>>=
# load data and library
options(warn = -1)
library(xtable)
library(png)
library(grid)
library(ggplot2)
library(reshape2)

# load data in our working environment 
load('../data/ols-regression.RData')
load('../data/ridge-regression.RData')
load('../data/lasso-regression.RData')
load('../data/PCR.RData')
load('../data/PLSR.RData')

# load our functions
source("../code/functions/mse-function.R")
@


<<coefficients, results=tex, echo=FALSE>>=
# Extract coefficients and convert them into vectors
osl_coef_vector <- osl_coef[1:12]
ridge_coef_vector <- unlist(ridge_coef)[1:12]
lasso_coef_vector <- unlist(lasso_coef)[1:12]
pcr_coef_vector <- c(0, pcr_coef[1:11])
plsr_coef_vector <- c(0, plsr_coef[1:11])

# Create a table of regression coefficients for all methods
table_coef <- data.frame("OLS" = osl_coef_vector, "Ridge" = ridge_coef_vector, 
                         "Lasso" = lasso_coef_vector, "PCR" = pcr_coef_vector,
                         "PLSR" = plsr_coef_vector)
rownames(table_coef) <- rownames(ridge_coef)

@


The following table contains the regression coefficients for all methods including: ols, ridge, lasso, pcr, and plsr. 

<<coefficients, results=tex, echo=FALSE>>=
# Use xtable to display our coefficients dataframe
xtable_coef <- xtable(table_coef, caption = 'Regression Coefficients for All Methods',
                      digits = c(0, 3, 3, 3, 3, 3))
print(xtable_coef, comment=FALSE, type = "latex", caption.placement = 'top')
@


From Table 6, the result shows that regression coefficients for ridge, lasso, pcr, and plsr are approximately closed to each other's value but a slightly different comparing to ols - our benchmark. 

Not surprisingly, we have seen that some coefficients in lasso regression are zero because lasso regression allows coefficients to be zero to minimize the regression penalty.

Now let's look at another table with the test MSE values for the regression techniques: ridge, lasso, pcr, and plsr.

<<coefficients, results=tex, echo=FALSE>>=
# Create test mse dataframe
table_mse <- data.frame("ridge" = ridge_test_mse, "lasso" = lasso_test_mse,
                        "PCR" = pcr_test_mse, "PLSR" = plsr_test_mse)
rownames(table_mse) <- "MSE Value"

# Use xtable to display our test mse dataframe
xtable_mse <- xtable(table_mse, caption = 'Test MSE for All Methods', 
                     digits = c(0, 3, 3, 3, 3))
print(xtable_mse, comment=FALSE, type = "latex", caption.placement = 'top')
@

Table 7 has only one row (Test MSE value) and four columns (one column per regression methods: ridge, lasso, pcr, and plsr).

From Table 7, the result shows that the model with lowest test Mean Square Error is Ridge Regression, which means that ridge regression acttually has the best performance when we test the prediction against the true value in testing set. So ridge regression is the best model in terms of measuring the fitness by MSE.

Now let's look at a plot in which the official coefficients are compared. We plot trend lines (i.e. the profiles) of the coefficients (one line connecting the coefficients of each method).

<<coefficients, results=tex, echo=FALSE>>=
# Create a new dataframe for coefficient in order to plot 
plot_table_coef <- cbind(data.frame("Predictors" = rownames(table_coef)), table_coef)
rownames(plot_table_coef) <- NULL

# Reshape our dataframe in order to plot multiple lines
plot_table_coef <- melt(plot_table_coef, id.vars = "Predictors" , value.name= "value", variable.name= "Methods")

# Use ggplot to acchieve multiple lines plot
ggplot(plot_table_coef, aes(x = Predictors, y=value, group = Methods, colour = Methods)) +
    geom_line() +
    geom_point(size = 4, shape = 21, fill = "white") +
    ggtitle("Figure 5: Official Coefficients Comparison") + theme_bw() + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
@

Figure 5 displays comparison of coefficients of each predictor between different methods that we discussed earlier. In this visualization, we again confirm that there is some level of similarity between all methods.



\end{document}